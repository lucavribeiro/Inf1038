{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 22:03:29.921904: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-03 22:03:29.921963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-03 22:03:29.923226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-03 22:03:29.930437: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import cpu_count\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Cons_rating</th>\n",
       "      <th>Cloth_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45303</th>\n",
       "      <td>49333</td>\n",
       "      <td>Dress felt and fit great.  I got lots of compl...</td>\n",
       "      <td>Loved the color!!!  Dress fit great and I got ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45304</th>\n",
       "      <td>49334</td>\n",
       "      <td>Loved the dress but poor quality</td>\n",
       "      <td>This dress looked great and I loved the materi...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45305</th>\n",
       "      <td>49335</td>\n",
       "      <td>Cute dress, didn't fit</td>\n",
       "      <td>Wanted this dress to work it didn't. It is ver...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45306</th>\n",
       "      <td>49336</td>\n",
       "      <td>Very cute!</td>\n",
       "      <td>No complaints othe than the zipper gets stuck ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45307</th>\n",
       "      <td>49337</td>\n",
       "      <td>Good fit</td>\n",
       "      <td>The fabric was really nice, I'm a L and it fit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45308 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Title  \\\n",
       "0               2                            Some major design flaws   \n",
       "1               3                                   My favorite buy!   \n",
       "2               4                                   Flattering shirt   \n",
       "3               5                            Not for the very petite   \n",
       "4               6                               Cagrcoal shimmer fun   \n",
       "...           ...                                                ...   \n",
       "45303       49333  Dress felt and fit great.  I got lots of compl...   \n",
       "45304       49334                   Loved the dress but poor quality   \n",
       "45305       49335                             Cute dress, didn't fit   \n",
       "45306       49336                                         Very cute!   \n",
       "45307       49337                                           Good fit   \n",
       "\n",
       "                                                  Review  Cons_rating  \\\n",
       "0      I had such high hopes for this dress and reall...          3.0   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...          5.0   \n",
       "2      This shirt is very flattering to all due to th...          5.0   \n",
       "3      I love tracy reese dresses, but this one is no...          2.0   \n",
       "4      I aded this in my basket at hte last mintue to...          5.0   \n",
       "...                                                  ...          ...   \n",
       "45303  Loved the color!!!  Dress fit great and I got ...          5.0   \n",
       "45304  This dress looked great and I loved the materi...          2.0   \n",
       "45305  Wanted this dress to work it didn't. It is ver...          1.0   \n",
       "45306  No complaints othe than the zipper gets stuck ...          4.0   \n",
       "45307  The fabric was really nice, I'm a L and it fit...          5.0   \n",
       "\n",
       "      Cloth_class  \n",
       "0         Dresses  \n",
       "1           Pants  \n",
       "2         Blouses  \n",
       "3         Dresses  \n",
       "4           Knits  \n",
       "...           ...  \n",
       "45303     Dresses  \n",
       "45304     Dresses  \n",
       "45305     Dresses  \n",
       "45306     Dresses  \n",
       "45307     Dresses  \n",
       "\n",
       "[45308 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepath = 'data/reviews.csv' if not kaggle else './kaggle/input/consumer-review-of-clothing-product/Consumer Review of Clothing Product/data_amazon.xlsx - Sheet1.csv'\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(name, model, X, y, grid={}):\n",
    "    print(name)\n",
    "\n",
    "    scoring = make_scorer(roc_auc_score, multi_class='ovo',needs_proba=True)\n",
    "    \n",
    "    res_grid = None\n",
    "    test_size = 0.4\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=test_size,random_state=42, stratify = y)\n",
    "    res = GridSearchCV(model,grid, scoring = scoring, n_jobs = 8)\n",
    "    res.fit(X_train,Y_train)\n",
    "    res_grid = res.best_params_\n",
    "\n",
    "    prob_train = res.predict_proba(X_train)\n",
    "    prob_test = res.predict_proba(X_test)\n",
    "    \n",
    "    scores = [roc_auc_score(Y_train, prob_train, multi_class = 'ovo'), roc_auc_score(Y_test, prob_test, multi_class = 'ovo')]\n",
    "    \n",
    "    print(\"Train score: \", scores[0])\n",
    "    print(\"Test score: \", scores[1])\n",
    "    print(\"Best params: \", res_grid)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "    models.append((name,scores[0],scores[1],res_grid))\n",
    "    return res,res_grid,scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = df['Review']\n",
    "\n",
    "words = \"\".join(word for word in word_features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {'english', 'spanish', 'portuguese', 'italian'}\n",
    "\n",
    "stopwords = list()\n",
    "\n",
    "for lang in languages:\n",
    "    stopwords.extend(nltk.corpus.stopwords.words(lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = word_features.str.split().apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "stemmed = stemmed.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words = stopwords, min_df = 3, max_df = 6000)\n",
    "\n",
    "text_all = tfidf_vect.fit_transform(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7603"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = list(df['Cloth_class'].unique())\n",
    "\n",
    "targets = df['Cloth_class']\n",
    "\n",
    "class_map = {cloth_class: target_classes.index(cloth_class) for cloth_class in target_classes} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_all\n",
    "y = targets.map(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45308, 7603)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Train score:  0.9426044043803135\n",
      "Test score:  0.8973464721065647\n",
      "Best params:  {'C': 1}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=LogisticRegression(max_iter=5000), n_jobs=4,\n",
       "              param_grid={'C': [0.1, 1, 10]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'C': 1},\n",
       " [0.9426044043803135, 0.8973464721065647])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"Logistic Regression\"\n",
    "logistic_grid = {'C':[0.1,1,10]}\n",
    "train_test(name,LogisticRegression(max_iter = 5000), X, y, logistic_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC rbf\n"
     ]
    }
   ],
   "source": [
    "name = 'SVC'\n",
    "\n",
    "svc_grid = {\n",
    "  'rbf': {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.001, 0.01, 0.1],\n",
    "    'kernel': ['rbf'],\n",
    "    'probability': [True]\n",
    "  },\n",
    "  'poly': {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'degree': [2, 3, 4],\n",
    "    'kernel': ['poly'],\n",
    "    'probability': [True]\n",
    "  },\n",
    "  'sigmoid': {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['sigmoid'],\n",
    "    'probability': [True]\n",
    "  }\n",
    "}\n",
    "\n",
    "kernel = 'rbf'\n",
    "t = train_test(f\"{name} {kernel}\", SVC(kernel=kernel), X, y, svc_grid[kernel])\n",
    "print(t)\n",
    "\n",
    "kernel = 'poly'\n",
    "t = train_test(f\"{name} {kernel}\", SVC(kernel=kernel), X, y, svc_grid[kernel])\n",
    "print(t)\n",
    "\n",
    "kernel = 'sigmoid'\n",
    "t = train_test(f\"{name} {kernel}\", SVC(kernel=kernel), X, y, svc_grid[kernel])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9582361880717837\n",
      "Test score:  0.8864348411059657\n",
      "Best params:  {'max_depth': 100, 'max_leaf_nodes': 1000, 'n_estimators': 600}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=RandomForestClassifier(), n_jobs=4,\n",
       "              param_grid={'max_depth': [50, 100, None],\n",
       "                          'max_leaf_nodes': [600, 800, 1000, None],\n",
       "                          'n_estimators': [300, 400, 500, 600]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'max_depth': 100, 'max_leaf_nodes': 1000, 'n_estimators': 600},\n",
       " [0.9582361880717837, 0.8864348411059657])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"Random Forest Classifier\"\n",
    "random_forest_grid = {'n_estimators':[300,400,500, 600], 'max_depth':[50,100, None], 'max_leaf_nodes': [600,800,1000,None]}\n",
    "train_test(name,RandomForestClassifier(), X, y, random_forest_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Train score:  0.9391052797915587\n",
      "Test score:  0.5640452633619534\n",
      "Best params:  {'n_neighbors': 10}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=KNeighborsClassifier(), n_jobs=4,\n",
       "              param_grid={'n_neighbors': [5, 7, 10, 15, 20]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'n_neighbors': 10},\n",
       " [0.9391052797915587, 0.5640452633619534])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"KNN\"\n",
    "knn_grid = {'n_neighbors':[5,7,10, 15, 20]}\n",
    "train_test(name,KNeighborsClassifier(),X,y,knn_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Train score:  0.914849467140905\n",
      "Test score:  0.8684332396182726\n",
      "Best params:  {}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=MultinomialNB(), n_jobs=4, param_grid={},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {},\n",
       " [0.914849467140905, 0.8684332396182726])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"Naive Bayes\"\n",
    "train_test(name,MultinomialNB(),X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Train score:  0.9387584644981929\n",
      "Test score:  0.7206114679195625\n",
      "Best params:  {'max_depth': 50}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=DecisionTreeClassifier(), n_jobs=4,\n",
       "              param_grid={'max_depth': [None, 50, 100]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'max_depth': 50},\n",
       " [0.9387584644981929, 0.7206114679195625])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"Decision Tree\"\n",
    "decision_tree_grid = {'max_depth':[None,50,100]}\n",
    "train_test(name,DecisionTreeClassifier(),X,y,decision_tree_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.885409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.871889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.805107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.919219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.192067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.930217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.891791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.385899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.777442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.312168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.115331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.647009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.399639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.914973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.430469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.018165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.881194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.191321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.053226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.887457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.087173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.316295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.268412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.114141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.050733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.962686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.041280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 7.745804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.310588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 3.655211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.985702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.609465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.689875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.750675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.002010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 109007\n",
      "[LightGBM] [Info] Number of data points in the train set: 27184, number of used features: 2068\n",
      "[LightGBM] [Info] Start training from score -1.644400\n",
      "[LightGBM] [Info] Start training from score -2.541823\n",
      "[LightGBM] [Info] Start training from score -2.088013\n",
      "[LightGBM] [Info] Start training from score -2.431754\n",
      "[LightGBM] [Info] Start training from score -2.211041\n",
      "[LightGBM] [Info] Start training from score -2.812210\n",
      "[LightGBM] [Info] Start training from score -2.277304\n",
      "[LightGBM] [Info] Start training from score -2.485937\n",
      "[LightGBM] [Info] Start training from score -2.707646\n",
      "[LightGBM] [Info] Start training from score -3.243417\n",
      "[LightGBM] [Info] Start training from score -2.897830\n",
      "Train score:  0.9660896714985653\n",
      "Test score:  0.895285645296603\n",
      "Best params:  {'learning_rate': 0.08, 'num_leaves': 30}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=LGBMClassifier(), n_jobs=4,\n",
       "              param_grid={'learning_rate': [0.06, 0.08, 0.1],\n",
       "                          'num_leaves': [10, 20, 30]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'learning_rate': 0.08, 'num_leaves': 30},\n",
       " [0.9660896714985653, 0.895285645296603])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"LightGBM\"\n",
    "lightgbm_grid = {'num_leaves':[10, 20, 30], 'learning_rate': [0.06, 0.08, 0.1]}\n",
    "train_test(name,lgb.LGBMClassifier(),X,y,lightgbm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.853163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.073762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.570646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.061438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "Train score:  0.9824808710472075\n",
      "Test score:  0.887187639040006\n",
      "Best params:  {'max_depth': 10}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                      callbacks=None, colsample_bylevel=None,\n",
       "                                      colsample_bynode=None,\n",
       "                                      colsample_bytree=None, device=None,\n",
       "                                      early_stopping_rounds=None,\n",
       "                                      enable_categorical=False, eval_metric=None,\n",
       "                                      feature_types=None, gamma=None,\n",
       "                                      grow_policy=None, importance_type=None,\n",
       "                                      interaction_constraints=None,\n",
       "                                      learning_rate=None, max_b...\n",
       "                                      max_cat_threshold=None,\n",
       "                                      max_cat_to_onehot=None,\n",
       "                                      max_delta_step=None, max_depth=None,\n",
       "                                      max_leaves=None, min_child_weight=None,\n",
       "                                      missing=nan, monotone_constraints=None,\n",
       "                                      multi_strategy=None, n_estimators=None,\n",
       "                                      n_jobs=None, num_parallel_tree=None,\n",
       "                                      random_state=None, ...),\n",
       "              n_jobs=4, param_grid={'max_depth': [10]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'max_depth': 10},\n",
       " [0.9824808710472075, 0.887187639040006])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"XGBoost\"\n",
    "xgboost_grid = {'max_depth':[10]}\n",
    "train_test(name,xgb.XGBClassifier(),X,y,xgboost_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb CÃ©lula 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCatBoost\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m catboost_grid \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39m10\u001b[39m,\u001b[39m100\u001b[39m,\u001b[39m1000\u001b[39m]}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_test(name,cb\u001b[39m.\u001b[39;49mCatBoostClassifier(verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),X,y,catboost_grid)\n",
      "\u001b[1;32m/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb CÃ©lula 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_train, X_test, Y_train, Y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39mtest_size,random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, stratify \u001b[39m=\u001b[39m y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m res \u001b[39m=\u001b[39m GridSearchCV(model,grid, scoring \u001b[39m=\u001b[39m scoring, n_jobs \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m res\u001b[39m.\u001b[39;49mfit(X_train,Y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb#X40sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m res_grid \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mbest_params_\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/edudaluna/HD/Repos/Github/Inf1038/luna.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m prob_train \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mpredict_proba(X_train)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "name = \"CatBoost\"\n",
    "catboost_grid = {'max_depth':[10,100,1000]}\n",
    "train_test(name,cb.CatBoostClassifier(verbose=False),X,y,catboost_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = \"HGBoost\"\n",
    "hgboost_grid = {'max_depth':[10,100,1000]}\n",
    "train_test(name,HistGradientBoostingClassifier(),X,y,hgboost_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn models into a dataframe\n",
    "models_df = pd.DataFrame(models, columns = ['Model','Train Score','Test Score','Best Params'])\n",
    "models_df.to_csv('models2.csv')\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(\u001b[43mY\u001b[49m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnn_3\u001b[39m():\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "output_dim = len(np.unique(y))\n",
    "\n",
    "def nn_3():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(16))\n",
    "    model.add(layers.Dense(output_dim, activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "name = \"Neural Network 2x64D\"\n",
    "\n",
    "train_test(name,KerasClassifier(),X,y,{\n",
    "    'epochs':[100,500],\n",
    "    'batch_size':[100,1000], \n",
    "    'callbacks':[EarlyStopping(patience=10)],\n",
    "    'model': [nn_3],\n",
    "    'verbose':[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Logistic Regression', 0.8551417809497813, 0.7864512067997768, {'C': 1}),\n",
       " ('SVC rbf',\n",
       "  0.8611685061527988,\n",
       "  0.7721698248032369,\n",
       "  {'C': 10, 'gamma': 0.1, 'kernel': 'rbf', 'probability': True}),\n",
       " ('SVC poly',\n",
       "  0.8827666995358318,\n",
       "  0.7565950928830414,\n",
       "  {'C': 1, 'degree': 2, 'kernel': 'poly', 'probability': True}),\n",
       " ('Random Forest Classifier',\n",
       "  0.8761751468633401,\n",
       "  0.7775741889507252,\n",
       "  {'max_depth': 100, 'max_leaf_nodes': 600, 'n_estimators': 500}),\n",
       " ('KNN', 0.829006394358123, 0.6837642921367546, {'n_neighbors': 10}),\n",
       " ('Naive Bayes', 0.841699767970446, 0.7768429334670092, {}),\n",
       " ('Decision Tree', 0.8846417949957864, 0.6799373364361562, {'max_depth': 100}),\n",
       " ('LightGBM', 0.8266517607814851, 0.7714653355113226, {'num_leaves': 10}),\n",
       " ('XGBoost', 0.8894987829872039, 0.7679569464180195, {'max_depth': 10}),\n",
       " ('CatBoost', 0.8376703900919972, 0.7816794454436, {'max_depth': 10})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_leaf_nodes = 504, n_estimators = 474, random_state = 1)\n",
    "tree = DecisionTreeClassifier(max_leaf_nodes = 504)\n",
    "\n",
    "log_r = LogisticRegression(max_iter = 8000)\n",
    "log_r2 = LogisticRegression(max_iter = 8000)\n",
    "\n",
    "reducer = TruncatedSVD(n_components = 1000)\n",
    "\n",
    "log_r_pipe = Pipeline([\n",
    "                ('reducer', reducer),\n",
    "                ('logistic_regression', log_r2)\n",
    "             ])\n",
    "\n",
    "estimators = [\n",
    "              ('random_forest', rf),\n",
    "              ('tree', tree),\n",
    "              ('logistic_regression1', log_r),    \n",
    "              ('logistic_regression2', log_r_pipe)\n",
    "            ]\n",
    "\n",
    "train_test(\"Voting Ensamble\",VotingClassifier(estimators = estimators, voting = 'soft', n_jobs = 4),X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.855142</td>\n",
       "      <td>0.786451</td>\n",
       "      <td>{'C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC rbf</td>\n",
       "      <td>0.861169</td>\n",
       "      <td>0.772170</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'rbf', 'prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC poly</td>\n",
       "      <td>0.882767</td>\n",
       "      <td>0.756595</td>\n",
       "      <td>{'C': 1, 'degree': 2, 'kernel': 'poly', 'proba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.876175</td>\n",
       "      <td>0.777574</td>\n",
       "      <td>{'max_depth': 100, 'max_leaf_nodes': 600, 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.829006</td>\n",
       "      <td>0.683764</td>\n",
       "      <td>{'n_neighbors': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.841700</td>\n",
       "      <td>0.776843</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.884642</td>\n",
       "      <td>0.679937</td>\n",
       "      <td>{'max_depth': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.826652</td>\n",
       "      <td>0.771465</td>\n",
       "      <td>{'num_leaves': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.889499</td>\n",
       "      <td>0.767957</td>\n",
       "      <td>{'max_depth': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.837670</td>\n",
       "      <td>0.781679</td>\n",
       "      <td>{'max_depth': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Train Score  Test Score  \\\n",
       "0       Logistic Regression     0.855142    0.786451   \n",
       "1                   SVC rbf     0.861169    0.772170   \n",
       "2                  SVC poly     0.882767    0.756595   \n",
       "3  Random Forest Classifier     0.876175    0.777574   \n",
       "4                       KNN     0.829006    0.683764   \n",
       "5               Naive Bayes     0.841700    0.776843   \n",
       "6             Decision Tree     0.884642    0.679937   \n",
       "7                  LightGBM     0.826652    0.771465   \n",
       "8                   XGBoost     0.889499    0.767957   \n",
       "9                  CatBoost     0.837670    0.781679   \n",
       "\n",
       "                                         Best Params  \n",
       "0                                           {'C': 1}  \n",
       "1  {'C': 10, 'gamma': 0.1, 'kernel': 'rbf', 'prob...  \n",
       "2  {'C': 1, 'degree': 2, 'kernel': 'poly', 'proba...  \n",
       "3  {'max_depth': 100, 'max_leaf_nodes': 600, 'n_e...  \n",
       "4                                {'n_neighbors': 10}  \n",
       "5                                                 {}  \n",
       "6                                 {'max_depth': 100}  \n",
       "7                                 {'num_leaves': 10}  \n",
       "8                                  {'max_depth': 10}  \n",
       "9                                  {'max_depth': 10}  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn models into a dataframe\n",
    "models_df = pd.DataFrame(models, columns = ['Model','Train Score','Test Score','Best Params'])\n",
    "models_df.to_csv('models3.csv')\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
