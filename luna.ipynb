{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 09:28:51.351039: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-04 09:28:51.351109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-04 09:28:51.388465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-04 09:28:51.483599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import cpu_count\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Cons_rating</th>\n",
       "      <th>Cloth_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45303</th>\n",
       "      <td>49333</td>\n",
       "      <td>Dress felt and fit great.  I got lots of compl...</td>\n",
       "      <td>Loved the color!!!  Dress fit great and I got ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45304</th>\n",
       "      <td>49334</td>\n",
       "      <td>Loved the dress but poor quality</td>\n",
       "      <td>This dress looked great and I loved the materi...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45305</th>\n",
       "      <td>49335</td>\n",
       "      <td>Cute dress, didn't fit</td>\n",
       "      <td>Wanted this dress to work it didn't. It is ver...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45306</th>\n",
       "      <td>49336</td>\n",
       "      <td>Very cute!</td>\n",
       "      <td>No complaints othe than the zipper gets stuck ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45307</th>\n",
       "      <td>49337</td>\n",
       "      <td>Good fit</td>\n",
       "      <td>The fabric was really nice, I'm a L and it fit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45308 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Title  \\\n",
       "0               2                            Some major design flaws   \n",
       "1               3                                   My favorite buy!   \n",
       "2               4                                   Flattering shirt   \n",
       "3               5                            Not for the very petite   \n",
       "4               6                               Cagrcoal shimmer fun   \n",
       "...           ...                                                ...   \n",
       "45303       49333  Dress felt and fit great.  I got lots of compl...   \n",
       "45304       49334                   Loved the dress but poor quality   \n",
       "45305       49335                             Cute dress, didn't fit   \n",
       "45306       49336                                         Very cute!   \n",
       "45307       49337                                           Good fit   \n",
       "\n",
       "                                                  Review  Cons_rating  \\\n",
       "0      I had such high hopes for this dress and reall...          3.0   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...          5.0   \n",
       "2      This shirt is very flattering to all due to th...          5.0   \n",
       "3      I love tracy reese dresses, but this one is no...          2.0   \n",
       "4      I aded this in my basket at hte last mintue to...          5.0   \n",
       "...                                                  ...          ...   \n",
       "45303  Loved the color!!!  Dress fit great and I got ...          5.0   \n",
       "45304  This dress looked great and I loved the materi...          2.0   \n",
       "45305  Wanted this dress to work it didn't. It is ver...          1.0   \n",
       "45306  No complaints othe than the zipper gets stuck ...          4.0   \n",
       "45307  The fabric was really nice, I'm a L and it fit...          5.0   \n",
       "\n",
       "      Cloth_class  \n",
       "0         Dresses  \n",
       "1           Pants  \n",
       "2         Blouses  \n",
       "3         Dresses  \n",
       "4           Knits  \n",
       "...           ...  \n",
       "45303     Dresses  \n",
       "45304     Dresses  \n",
       "45305     Dresses  \n",
       "45306     Dresses  \n",
       "45307     Dresses  \n",
       "\n",
       "[45308 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepath = 'data/reviews.csv' if not kaggle else './kaggle/input/consumer-review-of-clothing-product/Consumer Review of Clothing Product/data_amazon.xlsx - Sheet1.csv'\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(name, model, X, y, grid={}):\n",
    "    print(name)\n",
    "\n",
    "    scoring = make_scorer(roc_auc_score, multi_class='ovo',needs_proba=True)\n",
    "    \n",
    "    res_grid = None\n",
    "    test_size = 0.4\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=test_size,random_state=42, stratify = y)\n",
    "    res = GridSearchCV(model,grid, scoring = scoring, n_jobs = 8)\n",
    "    res.fit(X_train,Y_train)\n",
    "    res_grid = res.best_params_\n",
    "\n",
    "    prob_train = res.predict_proba(X_train)\n",
    "    prob_test = res.predict_proba(X_test)\n",
    "    \n",
    "    scores = [roc_auc_score(Y_train, prob_train, multi_class = 'ovo'), roc_auc_score(Y_test, prob_test, multi_class = 'ovo')]\n",
    "    \n",
    "    print(\"Train score: \", scores[0])\n",
    "    print(\"Test score: \", scores[1])\n",
    "    print(\"Best params: \", res_grid)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "    models.append((name,scores[0],scores[1],res_grid))\n",
    "    return res,res_grid,scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = df['Review']\n",
    "\n",
    "words = \"\".join(word for word in word_features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {'english', 'spanish', 'portuguese', 'italian'}\n",
    "\n",
    "stopwords = list()\n",
    "\n",
    "for lang in languages:\n",
    "    stopwords.extend(nltk.corpus.stopwords.words(lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = word_features.str.split().apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "stemmed = stemmed.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words = stopwords, min_df = 3, max_df = 6000)\n",
    "\n",
    "text_all = tfidf_vect.fit_transform(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7603"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = list(df['Cloth_class'].unique())\n",
    "\n",
    "targets = df['Cloth_class']\n",
    "\n",
    "class_map = {cloth_class: target_classes.index(cloth_class) for cloth_class in target_classes} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_all\n",
    "y = targets.map(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45308, 7603)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Train score:  0.9426044043803135\n",
      "Test score:  0.8973464721065647\n",
      "Best params:  {'C': 1}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=LogisticRegression(max_iter=5000), n_jobs=8,\n",
       "              param_grid={'C': [0.1, 1, 10]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'C': 1},\n",
       " [0.9426044043803135, 0.8973464721065647])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"Logistic Regression\"\n",
    "logistic_grid = {'C':[0.1,1,10]}\n",
    "train_test(name,LogisticRegression(max_iter = 5000), X, y, logistic_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC rbf\n",
      "Train score:  0.9235903030731365\n",
      "Test score:  0.8890468442351281\n",
      "Best params:  {'C': 10, 'gamma': 0.01, 'kernel': 'rbf', 'probability': True}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(GridSearchCV(estimator=SVC(), n_jobs=8,\n",
      "             param_grid={'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1],\n",
      "                         'kernel': ['rbf'], 'probability': [True]},\n",
      "             scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)), {'C': 10, 'gamma': 0.01, 'kernel': 'rbf', 'probability': True}, [0.9235903030731365, 0.8890468442351281])\n",
      "SVC poly\n",
      "Train score:  0.9881512484318783\n",
      "Test score:  0.8743302774706202\n",
      "Best params:  {'C': 1, 'degree': 2, 'kernel': 'poly', 'probability': True}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(GridSearchCV(estimator=SVC(kernel='poly'), n_jobs=8,\n",
      "             param_grid={'C': [0.1, 1, 10], 'degree': [2, 3, 4],\n",
      "                         'kernel': ['poly'], 'probability': [True]},\n",
      "             scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)), {'C': 1, 'degree': 2, 'kernel': 'poly', 'probability': True}, [0.9881512484318783, 0.8743302774706202])\n",
      "SVC sigmoid\n",
      "Train score:  0.9237603524939275\n",
      "Test score:  0.89013246380777\n",
      "Best params:  {'C': 1, 'kernel': 'sigmoid', 'probability': True}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(GridSearchCV(estimator=SVC(kernel='sigmoid'), n_jobs=8,\n",
      "             param_grid={'C': [0.1, 1, 10], 'kernel': ['sigmoid'],\n",
      "                         'probability': [True]},\n",
      "             scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)), {'C': 1, 'kernel': 'sigmoid', 'probability': True}, [0.9237603524939275, 0.89013246380777])\n"
     ]
    }
   ],
   "source": [
    "name = 'SVC'\n",
    "\n",
    "svc_grid = {\n",
    "  'rbf': {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.001, 0.01, 0.1],\n",
    "    'kernel': ['rbf'],\n",
    "    'probability': [True]\n",
    "  },\n",
    "  'poly': {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'degree': [2, 3, 4],\n",
    "    'kernel': ['poly'],\n",
    "    'probability': [True]\n",
    "  },\n",
    "  'sigmoid': {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['sigmoid'],\n",
    "    'probability': [True]\n",
    "  }\n",
    "}\n",
    "\n",
    "kernel = 'rbf'\n",
    "t = train_test(f\"{name} {kernel}\", SVC(kernel=kernel), X, y, svc_grid[kernel])\n",
    "print(t)\n",
    "\n",
    "kernel = 'poly'\n",
    "t = train_test(f\"{name} {kernel}\", SVC(kernel=kernel), X, y, svc_grid[kernel])\n",
    "print(t)\n",
    "\n",
    "kernel = 'sigmoid'\n",
    "t = train_test(f\"{name} {kernel}\", SVC(kernel=kernel), X, y, svc_grid[kernel])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9581990221704432\n",
      "Test score:  0.8864625704991309\n",
      "Best params:  {'max_depth': 100, 'max_leaf_nodes': 1000, 'n_estimators': 600}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=RandomForestClassifier(), n_jobs=8,\n",
       "              param_grid={'max_depth': [50, 100, None],\n",
       "                          'max_leaf_nodes': [600, 800, 1000, None],\n",
       "                          'n_estimators': [300, 400, 500, 600]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'max_depth': 100, 'max_leaf_nodes': 1000, 'n_estimators': 600},\n",
       " [0.9581990221704432, 0.8864625704991309])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"Random Forest Classifier\"\n",
    "random_forest_grid = {'n_estimators':[300,400,500, 600], 'max_depth':[50,100, None], 'max_leaf_nodes': [600,800,1000,None]}\n",
    "train_test(name,RandomForestClassifier(), X, y, random_forest_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Train score:  0.9391052797915587\n",
      "Test score:  0.5640452633619534\n",
      "Best params:  {'n_neighbors': 10}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=KNeighborsClassifier(), n_jobs=8,\n",
       "              param_grid={'n_neighbors': [5, 7, 10, 15, 20]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'n_neighbors': 10},\n",
       " [0.9391052797915587, 0.5640452633619534])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"KNN\"\n",
    "knn_grid = {'n_neighbors':[5,7,10, 15, 20]}\n",
    "train_test(name,KNeighborsClassifier(),X,y,knn_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Train score:  0.914849467140905\n",
      "Test score:  0.8684332396182726\n",
      "Best params:  {}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=MultinomialNB(), n_jobs=8, param_grid={},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {},\n",
       " [0.914849467140905, 0.8684332396182726])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"Naive Bayes\"\n",
    "train_test(name,MultinomialNB(),X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Train score:  0.9384053073251551\n",
      "Test score:  0.7203832275923768\n",
      "Best params:  {'max_depth': 50}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=DecisionTreeClassifier(), n_jobs=8,\n",
       "              param_grid={'max_depth': [None, 50, 100]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'max_depth': 50},\n",
       " [0.9384053073251551, 0.7203832275923768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"Decision Tree\"\n",
    "decision_tree_grid = {'max_depth':[None,50,100]}\n",
    "train_test(name,DecisionTreeClassifier(),X,y,decision_tree_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 8.158684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 82.023764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 24.475572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 79.623931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 31.363827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1834\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.087707\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 50.175752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 12.039639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 73.523775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 44.877277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 81.472760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 79.739822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 61.063795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 73.939770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 81.164660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 82.515868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 79.883872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 79.315557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 81.467567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 81.895894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91667\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1844\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 9.577975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 47.687720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91436\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1836\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431849\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.811741\n",
      "[LightGBM] [Info] Start training from score -2.277026\n",
      "[LightGBM] [Info] Start training from score -2.486149\n",
      "[LightGBM] [Info] Start training from score -2.707912\n",
      "[LightGBM] [Info] Start training from score -3.243172\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 51.318388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91069\n",
      "[LightGBM] [Info] Number of data points in the train set: 21748, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -1.644437\n",
      "[LightGBM] [Info] Start training from score -2.541860\n",
      "[LightGBM] [Info] Start training from score -2.088124\n",
      "[LightGBM] [Info] Start training from score -2.431895\n",
      "[LightGBM] [Info] Start training from score -2.210742\n",
      "[LightGBM] [Info] Start training from score -2.812553\n",
      "[LightGBM] [Info] Start training from score -2.277520\n",
      "[LightGBM] [Info] Start training from score -2.486195\n",
      "[LightGBM] [Info] Start training from score -2.707269\n",
      "[LightGBM] [Info] Start training from score -3.243218\n",
      "[LightGBM] [Info] Start training from score -2.897200\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 46.783676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 91130\n",
      "[LightGBM] [Info] Number of data points in the train set: 21747, number of used features: 1817\n",
      "[LightGBM] [Info] Start training from score -1.644391\n",
      "[LightGBM] [Info] Start training from score -2.541814\n",
      "[LightGBM] [Info] Start training from score -2.088078\n",
      "[LightGBM] [Info] Start training from score -2.431326\n",
      "[LightGBM] [Info] Start training from score -2.211116\n",
      "[LightGBM] [Info] Start training from score -2.812507\n",
      "[LightGBM] [Info] Start training from score -2.277474\n",
      "[LightGBM] [Info] Start training from score -2.485597\n",
      "[LightGBM] [Info] Start training from score -2.707223\n",
      "[LightGBM] [Info] Start training from score -3.244350\n",
      "[LightGBM] [Info] Start training from score -2.897988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 109007\n",
      "[LightGBM] [Info] Number of data points in the train set: 27184, number of used features: 2068\n",
      "[LightGBM] [Info] Start training from score -1.644400\n",
      "[LightGBM] [Info] Start training from score -2.541823\n",
      "[LightGBM] [Info] Start training from score -2.088013\n",
      "[LightGBM] [Info] Start training from score -2.431754\n",
      "[LightGBM] [Info] Start training from score -2.211041\n",
      "[LightGBM] [Info] Start training from score -2.812210\n",
      "[LightGBM] [Info] Start training from score -2.277304\n",
      "[LightGBM] [Info] Start training from score -2.485937\n",
      "[LightGBM] [Info] Start training from score -2.707646\n",
      "[LightGBM] [Info] Start training from score -3.243417\n",
      "[LightGBM] [Info] Start training from score -2.897830\n",
      "Train score:  0.9660896714985653\n",
      "Test score:  0.895285645296603\n",
      "Best params:  {'learning_rate': 0.08, 'num_leaves': 30}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=LGBMClassifier(), n_jobs=8,\n",
       "              param_grid={'learning_rate': [0.06, 0.08, 0.1],\n",
       "                          'num_leaves': [10, 20, 30]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'learning_rate': 0.08, 'num_leaves': 30},\n",
       " [0.9660896714985653, 0.895285645296603])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"LightGBM\"\n",
    "lightgbm_grid = {'num_leaves':[10, 20, 30], 'learning_rate': [0.06, 0.08, 0.1]}\n",
    "train_test(name,lgb.LGBMClassifier(),X,y,lightgbm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Train score:  0.9824808710472075\n",
      "Test score:  0.887187639040006\n",
      "Best params:  {'max_depth': 10}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                      callbacks=None, colsample_bylevel=None,\n",
       "                                      colsample_bynode=None,\n",
       "                                      colsample_bytree=None, device=None,\n",
       "                                      early_stopping_rounds=None,\n",
       "                                      enable_categorical=False, eval_metric=None,\n",
       "                                      feature_types=None, gamma=None,\n",
       "                                      grow_policy=None, importance_type=None,\n",
       "                                      interaction_constraints=None,\n",
       "                                      learning_rate=None, max_b...\n",
       "                                      max_cat_threshold=None,\n",
       "                                      max_cat_to_onehot=None,\n",
       "                                      max_delta_step=None, max_depth=None,\n",
       "                                      max_leaves=None, min_child_weight=None,\n",
       "                                      missing=nan, monotone_constraints=None,\n",
       "                                      multi_strategy=None, n_estimators=None,\n",
       "                                      n_jobs=None, num_parallel_tree=None,\n",
       "                                      random_state=None, ...),\n",
       "              n_jobs=8, param_grid={'max_depth': [10]},\n",
       "              scoring=make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo)),\n",
       " {'max_depth': 10},\n",
       " [0.9824808710472075, 0.887187639040006])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"XGBoost\"\n",
    "xgboost_grid = {'max_depth':[10]}\n",
    "train_test(name,xgb.XGBClassifier(),X,y,xgboost_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n"
     ]
    }
   ],
   "source": [
    "\n",
    "name = \"CatBoost\"\n",
    "catboost_grid = {'max_depth':[10,100,1000]}\n",
    "train_test(name,cb.CatBoostClassifier(verbose=False),X,y,catboost_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = \"HGBoost\"\n",
    "hgboost_grid = {'max_depth':[10,100,1000]}\n",
    "train_test(name,HistGradientBoostingClassifier(),X,y,hgboost_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn models into a dataframe\n",
    "models_df = pd.DataFrame(models, columns = ['Model','Train Score','Test Score','Best Params'])\n",
    "models_df.to_csv('models4.csv')\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(np.unique(y))\n",
    "\n",
    "def nn_3():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(16))\n",
    "    model.add(layers.Dense(output_dim, activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "name = \"Neural Network 2x64D\"\n",
    "\n",
    "train_test(name,KerasClassifier(),X,y,{\n",
    "    'epochs':[100,500],\n",
    "    'batch_size':[100,1000], \n",
    "    'callbacks':[EarlyStopping(patience=10)],\n",
    "    'model': [nn_3],\n",
    "    'verbose':[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_leaf_nodes = 504, n_estimators = 474, random_state = 1)\n",
    "tree = DecisionTreeClassifier(max_leaf_nodes = 504)\n",
    "\n",
    "log_r = LogisticRegression(max_iter = 8000)\n",
    "log_r2 = LogisticRegression(max_iter = 8000)\n",
    "\n",
    "reducer = TruncatedSVD(n_components = 1000)\n",
    "\n",
    "log_r_pipe = Pipeline([\n",
    "                ('reducer', reducer),\n",
    "                ('logistic_regression', log_r2)\n",
    "             ])\n",
    "\n",
    "estimators = [\n",
    "              ('random_forest', rf),\n",
    "              ('tree', tree),\n",
    "              ('logistic_regression1', log_r),    \n",
    "              ('logistic_regression2', log_r_pipe)\n",
    "            ]\n",
    "\n",
    "train_test(\"Voting Ensamble\",VotingClassifier(estimators = estimators, voting = 'soft', n_jobs = 4),X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn models into a dataframe\n",
    "models_df = pd.DataFrame(models, columns = ['Model','Train Score','Test Score','Best Params'])\n",
    "models_df.to_csv('models3.csv')\n",
    "models_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
